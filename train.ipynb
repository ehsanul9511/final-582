{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.98-cp39-cp39-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.98\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, TextDataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('toy_training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and validation\n",
    "train_df = df.sample(frac=0.8, random_state=42)\n",
    "val_df = df.drop(train_df.index).reset_index(drop=True)\n",
    "\n",
    "# convert to list of dicts\n",
    "train_examples = train_df.to_dict('records')\n",
    "val_examples = val_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 10.5MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 18.7MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 689/689 [00:00<00:00, 554kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 6.43G/6.43G [02:50<00:00, 37.8MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 116kB/s]\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"t5-small\" # Or other T5 models like \"t5-base\", \"t5-large\", etc.\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "model_name = \"gpt2-xl\" # Or other GPT-2 models like \"gpt2\", \"gpt2-medium\", \"gpt2-large\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     input_tokenized[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m output_tokenized[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m input_tokenized\n\u001b[0;32m---> 11\u001b[0m train_data_tokenized \u001b[39m=\u001b[39m [tokenize_data(example, tokenizer) \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m train_examples]\n\u001b[1;32m     12\u001b[0m val_data_tokenized \u001b[39m=\u001b[39m [tokenize_data(example, tokenizer) \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m val_examples]\n",
      "Cell \u001b[0;32mIn[31], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     input_tokenized[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m output_tokenized[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m input_tokenized\n\u001b[0;32m---> 11\u001b[0m train_data_tokenized \u001b[39m=\u001b[39m [tokenize_data(example, tokenizer) \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m train_examples]\n\u001b[1;32m     12\u001b[0m val_data_tokenized \u001b[39m=\u001b[39m [tokenize_data(example, tokenizer) \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m val_examples]\n",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m, in \u001b[0;36mtokenize_data\u001b[0;34m(example, tokenizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m input_text \u001b[39m=\u001b[39m example[\u001b[39m\"\u001b[39m\u001b[39minput_text\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m output_text \u001b[39m=\u001b[39m example[\u001b[39m\"\u001b[39m\u001b[39moutput_text\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m input_tokenized \u001b[39m=\u001b[39m tokenizer(input_text, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m\"\u001b[39;49m, max_length\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m output_tokenized \u001b[39m=\u001b[39m tokenizer(output_text, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, max_length\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m)\n\u001b[1;32m      8\u001b[0m input_tokenized[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m output_tokenized[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda/envs/final582/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2538\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2536\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2537\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2538\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2539\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2540\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda/envs/final582/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2644\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2624\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2625\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2626\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2641\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2642\u001b[0m     )\n\u001b[1;32m   2643\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2644\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m   2645\u001b[0m         text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2646\u001b[0m         text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2647\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2648\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2649\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2650\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2651\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2652\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2653\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2654\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2655\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2656\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2657\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2658\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2659\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2660\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2661\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2662\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2663\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda/envs/final582/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2708\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2687\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2688\u001b[0m \u001b[39mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[1;32m   2689\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2704\u001b[0m \u001b[39m        method).\u001b[39;00m\n\u001b[1;32m   2705\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2707\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m-> 2708\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_padding_truncation_strategies(\n\u001b[1;32m   2709\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2710\u001b[0m     truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2711\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2712\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2713\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2714\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2715\u001b[0m )\n\u001b[1;32m   2717\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_plus(\n\u001b[1;32m   2718\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   2719\u001b[0m     text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2735\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2736\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/envs/final582/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2443\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2441\u001b[0m \u001b[39m# Test if we have a padding token\u001b[39;00m\n\u001b[1;32m   2442\u001b[0m \u001b[39mif\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token_id \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m-> 2443\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2444\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2445\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2446\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpad_token\u001b[39m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m[PAD]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m})`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2447\u001b[0m     )\n\u001b[1;32m   2449\u001b[0m \u001b[39m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[1;32m   2450\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2451\u001b[0m     truncation_strategy \u001b[39m!=\u001b[39m TruncationStrategy\u001b[39m.\u001b[39mDO_NOT_TRUNCATE\n\u001b[1;32m   2452\u001b[0m     \u001b[39mand\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2455\u001b[0m     \u001b[39mand\u001b[39;00m (max_length \u001b[39m%\u001b[39m pad_to_multiple_of \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m   2456\u001b[0m ):\n",
      "\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "source": [
    "def tokenize_data(example, tokenizer):\n",
    "    input_text = example[\"input_text\"]\n",
    "    output_text = example[\"output_text\"]\n",
    "\n",
    "    input_tokenized = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    output_tokenized = tokenizer(output_text, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    input_tokenized[\"labels\"] = output_tokenized[\"input_ids\"]\n",
    "    return input_tokenized\n",
    "\n",
    "train_data_tokenized = [tokenize_data(example, tokenizer) for example in train_examples]\n",
    "val_data_tokenized = [tokenize_data(example, tokenizer) for example in val_examples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tokenized[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'column_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# convert to huggingface dataset format\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_data \u001b[39m=\u001b[39m TextDataset(tokenizer, train_data_tokenized, column_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m      3\u001b[0m val_data \u001b[39m=\u001b[39m TextDataset(tokenizer, val_data_tokenized, column_names\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'column_names'"
     ]
    }
   ],
   "source": [
    "# convert to huggingface dataset format\n",
    "# train_data = TextDataset(tokenizer, train_data_tokenized, column_names=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "# val_data = TextDataset(tokenizer, val_data_tokenized, column_names=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"toy_t5\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"logging_dir\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data_tokenized,\n",
    "    eval_dataset=val_data_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ehsanulkabir/miniconda/envs/final582/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 36/36 [03:37<00:00,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 217.4661, 'train_samples_per_second': 1.269, 'train_steps_per_second': 0.166, 'train_loss': 9.668910556369358, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=36, training_loss=9.668910556369358, metrics={'train_runtime': 217.4661, 'train_samples_per_second': 1.269, 'train_steps_per_second': 0.166, 'train_loss': 9.668910556369358, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example # 0\n",
      "Input:  <OP_NAME> Ken Bone </s> <OP_TITLE> I’m American citizen, undecided voter, loving husband Ken Bone, Welcome to the Bone Zone! AMA </s> <OP_POST> Hello Reddit,\n",
      "\n",
      "I’m just a normal guy, who spends his free time with his hot wife and cat in St. Louis. I didn’t see any of this coming, it’s been a crazy week. I want to make something good come out of this moment, so I’m donating a portion of the proceeds from my Represent T-Shirt campaign to the St. Patrick Center raising money to fight homelessness in St. Louis.\n",
      "\n",
      "I’m an open book doing this AMA at my desk at work and excited to answer America’s question.\n",
      "\n",
      "Please support the campaign and the fight on homelessness! [Represent.com/bonezone](https://represent.com/bonezone/)\n",
      "\n",
      "Proof: http://i.imgur.com/GdMsMZ9.jpg\n",
      "\n",
      "Edit: signing off now, just like my whole experience so far this has been overwhelmingly positive!  Special thanks to my Reddit brethren for sticking up for me when the few negative people attack.  Let's just show that we're better than that by not answering hate with hate.  Maybe do this again in a few weeks when the ride is over if you have questions about returning to normal.\n",
      "\n",
      "My client will be answering no further questions.\n",
      "\n",
      "NEW EDIT:  This post is about to be locked, but questions are still coming in.  I made a new AMA to keep this going.  [You can find it here!](https://redd.it/63posi)\n",
      "\n",
      " </s> <OP_BIO> Kenneth Walter Bone (born May 21, 1958) is an American basketball coach, currently the associate head coach at Pepperdine University. </s> Questions from the Reddit:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ehsanulkabir/miniconda/envs/final582/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  <pad><extra_id_0> <unk>OP_NAME> Ken Bone (born May 21, 1958) is\n",
      "Expected output:  What was the process in order to actually ask the candidates your question? Did you sign a form, request using email yada yada yada.\n",
      "Hey Ken,  Can't wait to be you for Halloween.   You're obviously really passionate about serving the homeless community. Is there any background as to why that is your charity of choice?\n",
      "What has been the strangest thing someone tweeted at you in the wake of the debate?\n",
      "Are you near or farsighted?\n",
      "Is the best part of this whole ordeal the karma from this post?\n",
      "Are you looking forward to your impersonation on SNL?\n",
      "I love how you're using this moment for good. What does the St.Patrick center do for the homeless and why is this issue out of many the most important to you?\n",
      "What will you be going as for Halloween? I hear the sexy Ken Bone will be popular\n",
      "What do you think of Obama?\n",
      "As a coal worker, how do you think environmental protection and energy production should be balanced?\n",
      "Ken, you seem like a happy person who has their life together. Any life advice for a young adult?\n",
      "What are your feelings on [this thread](https://www.reddit.com/r/photoshopbattles/comments/57806q/psbattle_ken_bone_with_his_new_tshirt/)?\n",
      "How do you feel about becoming famous overnight? Also, have any companies approached you to endorse their products?\n",
      "What's the weirdest reaction from friends and family that you've had so far?\n",
      "How do you feel about the memes made about you?\n",
      "Hey Ken, who are some of your favorite musicians? \n",
      "Ken,  If you were president, what steps would your energy policy take to meet our energy needs while at the same time remaining environmentally friendly and minimizing job layoffs?\n",
      "What did Bill Clinton say to you after the debate?\n",
      "Ken, both your Twitter & Reddit handles end in the number 18...    Does the number 18 hold any personal significance to you?\n",
      "Which bone is connected to the Ken Bone?\n",
      "\n",
      "Example # 1\n",
      "Input:  <OP_NAME> Bill Gates </s> <OP_TITLE> I’m Bill Gates, co-chair of the Bill & Melinda Gates Foundation. Ask Me Anything. </s> <OP_POST> I’m excited to be back for my seventh AMA. I’ve learned a lot from the Reddit community over the past year (check out this fascinating [thread](https://www.reddit.com/r/robotics/comments/adxzo7/what_areas_of_robotics_research_are_you_most/) on robotics research), and I can’t wait to answer your questions.\n",
      "\n",
      "If you’re wondering what I’ve been up to (besides waiting in line for hamburgers), I recently [wrote](https://www.gatesnotes.com/About-Bill-Gates/Year-in-Review-2018?WT.mc_id=02_25_2019_10_AL2019_BG-RE_&WT.tsrc=BGRE) about what I learned at work last year.\n",
      "\n",
      "Melinda and I also just [published](https://www.gatesnotes.com/2019-Annual-Letter?WT.mc_id=02_25_2019_10_AL2019_BG-RE_&WT.tsrc=BGRE) our 11th Annual Letter. We wrote about nine things that have surprised us and inspired us to take action.\n",
      "\n",
      "One of those surprises, for example, is that Africa is the youngest continent. Here is an [infographic](https://www.reddit.com/r/Infographics/comments/aulqu1/africa_is_the_youngest_continent/) I made to explain what I mean.\n",
      "\n",
      "**Proof:** [https://](https://old.reddit.com/user/thisisbillgates/comments/auo4qn/cant_wait_to_kick_off_my_seventh_ama/)[reddit.com/user/thisisbillgates/comments/auo4qn/cant\\_wait\\_to\\_kick\\_off\\_my\\_seventh\\_ama/](https://old.reddit.com/user/thisisbillgates/comments/auo4qn/cant_wait_to_kick_off_my_seventh_ama/)\n",
      "\n",
      "**Edit:** I have to sign-off soon, but I’d love to answer a few more questions about energy innovation and climate change. If you post your questions [here](https://www.reddit.com/user/thisisbillgates/comments/auosel/ask_me_about_energy_innovation_and_climate_change/), I’ll answer as many as I can later on.\n",
      "\n",
      "**Edit:** Although I would love to stay forever, I have to get going. Thank you, Reddit, for another great AMA:  [https://imgur.com/a/kXmRubr](https://imgur.com/a/kXmRubr) </s> <OP_BIO> William Henry Gates III  (born October 28, 1955) is an American business magnate, philanthropist, and investor. He is best known for co-founding software giant Microsoft, along with his late childhood friend Paul Allen. During his career at Microsoft, Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect, while also being its largest individual shareholder until May 2014. He was a major entrepreneur of the microcomputer revolution of the 1970s and 1980s.\n",
      "Gates was born and raised in Seattle. In 1975, he and Allen founded Microsoft in Albuquerque, New Mexico. It became the world's largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, succeeded by Steve Ballmer, but he remained chairman of the board of directors and became chief software architect. During the late 1990s, he was criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2008, Gates transitioned to a part-time role at Microsoft and full-time work at the Bill & Melinda Gates Foundation, the private charitable foundation he and his then-wife Melinda established in 2000. He stepped down as chairman of the board of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella. In March 2020, Gates left his board positions at Microsoft and Berkshire Hathaway to focus on his philanthropic efforts on climate change, global health and development, and education.Since 1987, Gates has been included in the Forbes list of the world's wealthiest people. From 1995 to 2017, he held the Forbes title of the richest person in the world every year except from 2010 to 2013. In October 2017, he was surpassed by Amazon founder and CEO Jeff Bezos, who had an estimated net worth of US$90.6 billion compared to Gates's net worth of US$89.9 billion at the time. As of March 2023, Gates has an estimated net worth of US$116 billion, making him the fourth-richest person in the world according to Bloomberg News.Later in his career and since leaving day-to-day operations at Microsoft in 2008, Gates has pursued many business and philanthropic endeavors. He is the founder and chairman of several companies, including BEN, Cascade Investment, TerraPower, bgC3, and Breakthrough Energy. He has given sizable amounts of money to various charitable organizations and scientific research programs through the Bill & Melinda Gates Foundation, reported to be the world's largest private charity. Through the foundation, he led an early 21st century vaccination campaign that significantly contributed to the eradication of the wild poliovirus in Africa. In 2010, Gates and Warren Buffett founded The Giving Pledge, whereby they and other billionaires pledge to give at least half of their wealth to philanthropy. </s> Questions from the Reddit:\n",
      "Output:  <pad><extra_id_0> a<extra_id_1>I’m excited to be able to answer your questions. I’\n",
      "Expected output:  Through it all...What makes you happy?  (Looking for ideas)\n",
      "what would you still like to achieve that you haven’t?\n",
      "When was the last time you sat in front of a computer and wrote code?\n",
      "It’s well known you are an avid reader - what are a few books that come to your mind when asked to recommend reading materials for anyone that can have a true impact on their life - either professionally or personally?  (It’s awesome to see you here too!)  Thank you!\n",
      "What’s a piece of technology that’s theoretical now that you wish you could make possible immediately? Thanks in advance and thank you for all of your humanitarian efforts!\n",
      "The internet is fascinated by seeing you do \"normal\" things, like wait in line for a burger. But whats the most \"treat yo self\" rich guy thing that you do?  \n",
      "what's your favorite prime number?\n",
      "Do you think being a billionaire has made you a happier person than if you were just a middle class person?\n",
      "Do you think you could jump that chair 20 years later ?\n",
      "If you could go back in time and give your younger self advice. What would you say? \n",
      "What do you think the greatest threat to humanity is at this moment?\n",
      "Hi Bill, do you support ranked choice voting? Why or why not?   Thank you for your time today. \n",
      "Are you happy?\n",
      "What's the most encouraging bit of progress your foundation has funded to date?\n",
      "Hi, Bill!  As a frequent participant of Secret Santa, do you receive gifts back? What has been your favorite?  If you don't, who has been your favorite giftee?\n",
      "Hello, how's your day going and what have you been watching on Netflix and/or TV right now? \n",
      "Hi Mr Gates.  I work in health-care in South Africa and I just want to say thank you for the work that your foundation has been doing for HIV research here. Your willingness to help those who have less than you is something that I truly admire and aspire to.  My question is how do I feel like I'm doing enough? With all the pain and suffering that I see everyday, it's hard to feel like I am actually making a difference in the grand scheme of things. How do you deal with feeling like it's a never ending struggle to actually make a difference and help people?  Edit: I've noticed a lot of people giving gold and silver in this ama. Please consider donating the money instead to worthy causes. I'm sure Mr Gates would appreciate that a lot more. Thank you. And a big thank you to everyone for their kind words, I really do appreciate it.\n",
      "Hello Mr Gates.   Thank you for doing this AMA.   What are some common issues in poorer countries that isn't being talked about but needs attention?  What can the common person do other than donate money that would actually help these communities?  Thanks. -Martles\n",
      "What do you think of privatization of Space Industry?\n",
      "Any news on Age of Empires 4? \n",
      "\n",
      "Example # 2\n",
      "Input:  <OP_NAME> Max Brooks </s> <OP_TITLE> I am Max Brooks, author of World War Z, and I am here to discuss the coronavirus. Let’s talk about why my fictional zombie book was banned by the very real government of China. AMA. </s> <OP_POST> Let’s talk about survival. Individuals, groups, nations. Let’s talk about how fictional threats can teach us real survival skills. Let’s talk about why my fictional zombie book, “World War Z” was [banned by the very real government of China](https://www.washingtonpost.com/outlook/china-barred-my-dystopian-novel-about-how-its-system-enables-epidemics/2020/02/27/cc0446f0-58e5-11ea-9000-f3cffee23036_story.html) and how that government has let another very real plague get out of control. No matter what I write about, zombies, World War 1, Minecraft, and even my new threat, Bigfoot, the theme is always the same: adapting to survive. Let’s talk about what it means to adapt to this new Coronavirus danger and what it will mean for all of us.\n",
      "\n",
      "Proof: https://twitter.com/maxbrooksauthor/status/1237174231642734593 </s> <OP_BIO> Maximillian Michael Brooks (born May 22, 1972) is an American actor and author. He is the son of comedy filmmaker Mel Brooks and actress Anne Bancroft. Much of Brooks's writing focuses on zombie stories. He is a senior fellow at the Modern War Institute at West Point, New York. </s> Questions from the Reddit:\n",
      "Output:  <pad><extra_id_0>.<extra_id_1>,<extra_id_2> is a comedian and author of the American War.<extra_id_3>Max Brook\n",
      "Expected output:  I have a signature on my copy of Zombie Survival Guide from you with a quote.  'Destroy the Stairs.'  How useful is that atm? Should I wait?\n",
      "Hi Max, greetings from Madrid, Spain! Have you received any other complaints (official or unofficial) from any of the other governments described negatively in the book? For example, North Korea and what in the book citizens are forced to do.\n",
      "Why do you think they banned your book?\n",
      "I'm two thirds of the way through World War Z at the moment and I'm loving every bit of it so seeing this AMA was a very pleasant surprise!  My question: what are your thoughts on how little action many of the world's nations have been taking in dealing with this epidemic even though, unlike the plague in the book, coronavirus has been very widely reported on. Should we not have understood this would have gone global when it infected an entire province of china with a metropolitan area of 60+ million people?\n",
      "I hate to be that guy and go off subject, but do you have any more books like World War Z in pipeline? It’s one of my favorites and I would love for more of those amazing short stories. Thanks!\n",
      "Loved your book, read it cover to cover.  My favorite chapter was the downed pilot who survived in the marshes talking to her, possibly fictional, guide over the radio.  My question, how accurate do you think you captured the reactions of different countries?  Are there any corrections or additions you would make?  For example, I think in light of recent events, multinational criticism and blame would be a new dimension to talk about in World War Z, although I think you went for a more optimistic ending.\n",
      "What do you *really* think is happening in North Korea right now?\n",
      "I loved the way you created the story on finding patient 0 and how it all unfolded during that search. Do you see any effort done by govt's even interested in initial point entry to their countries, and have you been hit up by anyone asking for help?  Loved your book. Have you read up on the lore on the dollar flu in The storyline for Tom Clancy's the Division video game? I was reading/playing that at the near exact time I was also reading WarZ and the public response, apathy were eerily similar for works of fiction..\n",
      "What are the top takeaways from your books that we can use during the current pandemic? More importantly, how's your dad doing? How often do you visit him?\n",
      "With the number of rising cases around the world. Do you think the full quarantine shut downs (don't leave your house) are the extreme or the right call at this point in time?  Edit: as a side note. Thank you so much for Minecraft: The Island. I am a divorced father and bonding with my son long distance was hard. Reading that book with my son really helped close that gap. So thank you.\n",
      "Max, could you let us know how you feel about how Hollywood botched the movie adaptation of your fantastic book? I know it happens to many authors but I'd like your perspective.\n",
      "I loved your book Max!  Any plans to take that to a smaller format like a Netflix series and get into the stories of the individuals?  I really loved the blind swordsman story myself.\n",
      "Has there ever been talks to make WWZ into a miniseries on HBO or Netflix or something like that?  The book seems perfect for a miniseries or limited series format and it really deserves a better screen presentation than the movie we got.\n",
      "What type of research did you do for your fictional landscapes that you feel is important to share?\n",
      "Do you think a Redeker plan would be effective in a non-zombie pandemic scenario? (By that I mean the isolation tactics, the selective protection zones and the withdrawal of services from over-taxed areas to save strategically important ones?)\n",
      "Aside from being a well-written, engaging, even commanding, story, this novel may be the best socio-political thing I've ever read - fiction or otherwise. I feel strongly enough about this that I am going to try to start teaching this novel in my Advanced Literature class in high school.   What was your initial motivation in writing this story, and why did the movie break my heart?\n",
      "Big fan of WWZ and ZSG. Massive inspirations for my creative work.  I've always wanted a faux documentary tv adaption of WWZ. The film is fun but very different. I want to see a talking heads documentary with dramatised bits.  Has that ever come close to production? Sounds like a perfect candidate for an Amazon or Netflix mini-series.\n",
      "What actions would you take, if you were the leader of a small country with few infections so far?\n",
      "How do you think the response to the corona virus has been compared to the outbreak in your book?  The initial cover ups and then media frenzy and misinformation seem like I’m rereading sections of of the book. By the way World War Z is my all time favorite book!\n",
      "Which country as a whole—right now—do you personally view as having best handled coronavirus among its populace?  Huge fan of WWZ! Bought it on Kindle years back but haven't read it since then. Just re-downloaded it for another read!\n",
      "\n",
      "Example # 3\n",
      "Input:  <OP_NAME> Daniel Radcliffe </s> <OP_TITLE> I am Daniel Radcliffe. AMA! </s> <OP_POST> Hello, Daniel Radcliffe here. \n",
      "\n",
      "Proof: http://imgur.com/a/Pboxz\n",
      "\n",
      "My latest film is called \"Horns\" and it's in theaters October 31st. \n",
      "\n",
      "* Trailer: http://www.imdb.com/video/imdb/vi1949871129\n",
      "* Site: http://radiustwc.com/releases/horns/\n",
      "\n",
      "Victoria's assisting me with today's AMA. Hopefully I'll say something interesting.\n",
      "\n",
      "**Update**: Thank you very very much to everybody. Your questions have been awesome. But I really have to pee now. So we'll have to do this again sometime. \n",
      "\n",
      "And that is all true. \n",
      "\n",
      "But thank you very much, this has been great! </s> <OP_BIO> Daniel Jacob Radcliffe (born 23 July 1989) is an English actor. He rose to fame at age twelve, when he began portraying Harry Potter in the film series of the same name. Over his career, Radcliffe has received various awards and nominations.\n",
      "Radcliffe made his acting debut at age 10 in the BBC One television film David Copperfield (1999), followed by his feature film debut in The Tailor of Panama (2001). The same year, he starred as Harry Potter in the film adaptation of the J.K. Rowling fantasy novel, Harry Potter and the Philosopher's Stone. Over the next decade, he played the eponymous role in seven sequels, culminating with Harry Potter and the Deathly Hallows – Part 2 (2011). During this period, he became one of the world's highest-paid actors and gained worldwide fame, popularity, and critical acclaim.\n",
      "Following the success of Harry Potter, Radcliffe challenged himself acting in a variety of genres such as the romantic comedy What If? (2013), the horror films The Woman in Black (2012) and Victor Frankenstein (2015), the comedy-drama film Swiss Army Man (2016), the heist thriller film Now You See Me 2 (2016), and the action comedy The Lost City (2022). He earned critical acclaim for his portrayals of poet Allen Ginsberg in the drama film Kill Your Darlings (2013), FBI agent Nate Foster in the thriller film Imperium (2016), and Weird Al Yankovic in the comedy Weird: The Al Yankovic Story (2022) earning a Critics' Choice Television Award for the latter. Since 2019, he has starred in the TBS anthology series Miracle Workers.\n",
      "Radcliffe branched out to stage acting in 2007, starring in the West End and Broadway productions of Equus. From 2011 to 2012 he portrayed J. Pierrepont Finch in the Broadway revival of the musical How to Succeed in Business Without Really Trying. He continued in Martin McDonagh's dark comedy The Cripple of Inishmaan (2013–2014) in the West End and Broadway and a revival of Tom Stoppard's play Rosencrantz and Guildenstern Are Dead (2017) at The Old Vic. He also starred in the satirical plays Privacy (2016) and The Lifespan of a Fact (2018), respectively off and on Broadway. In 2022, he starred in the New York Theatre Workshop revival of Stephen Sondheim's Merrily We Roll Along.\n",
      "Radcliffe has contributed to many charities, including Demelza Hospice Care for Children and the Trevor Project; the latter awarded him its Hero Award in 2011. </s> Questions from the Reddit:\n",
      "Output:  <pad><extra_id_0>.<extra_id_1> <unk>OP_NAME> Daniel Radcliffe is a prolific actor\n",
      "Expected output:  Do you prefer Dan, Daniel, Danny... what?\n",
      "If you were to change one thing about the world what would it be and do you think your fame would help?\n",
      "Which actors do you want to work with in the future? \n",
      "What does Hedwig do now that the movies are all over?\n",
      "Hello Daniel! If you could be any book or comic character beside Harry Potter, who would you be?\n",
      "Hey Daniel! What are you going to be for Halloween?\n",
      "Not including Harry Potter, who is your favorite character in the franchise?\n",
      "Daniel, did you keep anything from the set of Harry Potter?\n",
      "If you had a Horcrux, what would it be?\n",
      "What is the weirdest thing someone has asked you to sign? \n",
      "Something I've been really impressed with lately is that it seems like all of the kids that grew up acting in Harry Potter ended up very grounded and intelligent (this came to mind in particular when I saw an excerpt of an interview where you questioned how quickly people sexualized Emma Watson as soon as she turned 18). Was there an environment that helped with that, or would you attribute that more to your parents, or something else?\n",
      "What movies/tv shows/podcasts/anything make you laugh the hardest?\n",
      "What was the best prank you pulled on the HP set? \n",
      "How terrifying was it to have a black magic version of Hans Gruber yelling at you for years?\n",
      "What was it like working with Richard Harris?\n",
      "Over the course of the Harry Potter films, you got to work with some amazing actors like Gary Oldman and Alan Rickman. Can you share any interesting stories about working with them?\n",
      "What is one thing in your life that you are striving to improve, and what are you doing to improve it?\n",
      "Howdy Daniel! Does Rupert really own an ice cream van?\n",
      "Hi Daniel, if the opportunity presents itself some day one day what would you prefer, play James Bond or play the villain?\n",
      "Harry Potter is obviously a Gryffindor, but what is Daniel Radcliffe?\n",
      "\n",
      "Example # 4\n",
      "Input:  <OP_NAME> Gordon Ramsay </s> <OP_TITLE> Hi guys! It’s Gordon Ramsay, back for another AMA, this time from London! There's a lot of exciting things happening in 2016, new restaurants, a mobile game…...so Ask Me Anything! And for my American fans, try not to overcook your burgers next weekend! </s> <OP_POST> I'm an [award-winning chef and restaurateur with 30 restaurants worldwide](http://www.gordonramsay.com/). Also known for presenting television programs, including Hell's Kitchen, MasterChef, MasterChef Junior, and Hotel Hell.\n",
      "\n",
      "I just launched my very first mobile game [#GordonRamsayDASH](http://hyperurl.co/gr_ama) where you get to build your very own restaurant empire, with yours truly as your guide!! It’s available now for download on the App store and Google Play. I hope everyone has as much fun playing as we did making it!\n",
      "\n",
      "[Proof](https://twitter.com/GordonRamsay/status/747442690019692545)\n",
      "\n",
      "Edit: \n",
      "\n",
      "Hi guys, just a quick apology for the ones I couldn't answer! I love doing this kind of stuff because that's how I am! I'd love to go live with you guys 7 days a week, my issue is time, I need one more day a week and 4 more hours in my 24 hours! I promise somewhere along the line I will get those questions answered. In the meantime, please, promise me one thing; Donald Trump will not be running America!  </s> <OP_BIO> Gordon James Ramsay  (; born (1966-11-08)8 November 1966) is a British celebrity chef, restaurateur, television personality and writer. His restaurant group, Gordon Ramsay Restaurants, was founded in 1997 and has been awarded 17 Michelin stars overall; it currently holds a total of seven. His signature restaurant, Restaurant Gordon Ramsay in Chelsea, London, has held three Michelin stars since 2001. After rising to fame on the British television miniseries Boiling Point in 1999, Ramsay became one of the best-known and most influential chefs in the world.Ramsay's television appearances are defined by his bluntness, fiery temper, strict demeanour, and the frequent use of profanity while making blunt, critical, and controversial comments, including insults and sardonic wisecracks about contestants and their cooking abilities. He combines activities in the television, film, hospitality, and food industries, and has promoted and hired various chefs who have apprenticed under his wing. He is known for presenting television programmes about competitive cookery and food, such as the British series Hell's Kitchen (2004), Ramsay's Kitchen Nightmares (2004–2009, 2014), and The F Word (2005–2010), with Kitchen Nightmares winning the 2005 British Academy Television Award for Best Feature, and the American versions of Hell's Kitchen (2005–present), Kitchen Nightmares (2007–2014), MasterChef (2010–present), and MasterChef Junior (2013–present), as well as Hotel Hell (2012–2016), Gordon Behind Bars (2012), Gordon Ramsay's 24 Hours to Hell and Back (2018–2020), and Next Level Chef (2022–present).\n",
      "Ramsay was appointed an OBE by Queen Elizabeth II in the 2006 New Year Honours list for services to the hospitality industry. He was named the top chef in the UK at the 2000 Catey Awards, and in July 2006 he won the Catey for Independent Restaurateur of the Year, becoming only the third person to have won three Catey Awards. In 2020, Forbes listed his earnings at US $70 million for the previous 12 months and ranked him at No. 19 on its list of the highest-earning celebrities. </s> Questions from the Reddit:\n",
      "Output:  <pad>Gordon Ramsay is a chef, restaurateur, chef and writer. he has\n",
      "Expected output:  In your opinion, what are 5 dishes that everyone needs to know how to cook?\n",
      "Hi Gordon, do you ever just wander into a restaurant looking for a bite to eat, if so how do you decide where to go? If so, when you wander in do the staff visibly begin shitting themselves? \n",
      "Hi, Gordon. You're my hero. My girlfriend is typically not much of an adventurous eater; she is, however, curious to try casu marzu or \"maggot cheese\". Having witnessed you try it on *The F Word*, I have to ask: is she bonkers or am I?\n",
      "What do you use a microwave for?\n",
      "Hi Mr Ramsay,  I recently went to your restaurant \"Gordon Ramsay Steak\" at Paris in Las Vegas. I was wondering, exactly how much influence do you hold there? Is it a name sake or do you supervise all of the production and menu, etc?\n",
      "https://youtu.be/3u-nQD-nwK8  In this video you're seen offering a job to an inmate after he beat you in an onion slicing contest. I was wondering if you could offer a follow up on this. Did he ever get the job?  Also I think it's a wonderful thing you did that day.\n",
      "What is the dumbest trend in food that you thought would not have lasted, but has?\n",
      "out of all of your shows which one is your favorite?\n",
      "What, in your opinion, is the easiest dish to get wrong, and how can you avoid it?  Also, what was the most pleasantly surprised you've ever been with something you've been served (whether it was the place you were eating, the name of the dish, etc). \n",
      "Will you ever open another location in Glasgow? The meal I had at Amaryllis remains hands down the best I've ever eaten.\n",
      "Can you call me a fucking donkey please?\n",
      "Hey Gordon -- I hope your leg is doing better.  Is there any dish a contestant has made on your Masterchef or Masterchef Junior that you still think about?  If so, have you ever used that contestants dish for creative influence in your own cooking?  Thank you. \n",
      "Chef what was your favorite moment on Masterchef, on or off camera?      Enjoyed your previous AMAs a lot, big fan sir.\n",
      "Do you think the cook off between you and Bobby Flay will ever happen?\n",
      "What was the inspiration to use rubber ducks for [the recent duck dish challenge on Hell's Kitchen](http://i.imgur.com/j4nvJAR.gifv)? What are your thoughts on the gimmicky challenges in general?\n",
      "Hey Gordon!   1) What's the one thing you have to order if you see it on a restaurant menu? 2) In light of the July 4th holiday next week, what's your ultimate burger blend? All beef or a blend of different proteins? 3) What chef did you most enjoy working for? Watching Marco Pierre White back in the day was hilarious!\n",
      "Are you happy with how you're perceived by most people?  Everyone views you as the strict, takes-no-excuses, and frankly, mean chef.  After seeing scenes like [this](https://www.youtube.com/watch?v=i0zRSANWj1I) or [this](https://www.youtube.com/watch?v=YsdLIpGsGno) from your Master Chef series, it's very clear that you have a lot of compassion for fellow chefs.  Do you think your tough, angry chef personality is often overplayed on your shows?\n",
      "Hey Gordon! Huge fan here, I watch a lot of your videos on YouTube. Being a poor college student, what easy meals would you recommend that are healthy and packed with nutrients for strong muscle development and faster mental cognition?\n",
      "Hi Chef Ramsey, do you have a favorite beer?\n",
      "Hello! I'm a huge fan of your shows, and thanks for doing so many [edit: 2] AMAs!  In hotel hell, which hotel was the worst that you've ever had to fix?   Also, which restaurant/hotel are you most proud of fixing?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Example #\", i)\n",
    "    print(\"Input: \", val_examples[i][\"input_text\"])\n",
    "    print(\"Output: \", tokenizer.decode(model.generate(tokenizer(val_examples[i][\"input_text\"], return_tensors=\"pt\").input_ids)[0]))\n",
    "    print(\"Expected output: \", val_examples[i][\"output_text\"])\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final582",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
